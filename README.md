# üé∞ TRADICIONAL BET DASHBOARD - Documenta√ß√£o T√©cnica Completa

## üìã √çNDICE

1. [Vis√£o Geral da Arquitetura](#1-vis√£o-geral-da-arquitetura)
2. [Arquitetura do Sistema](#2-arquitetura-do-sistema)
3. [Algoritmo 1: Isolation Forest](#3-algoritmo-1-isolation-forest)
4. [Algoritmo 2: SVD (Sistema de Recomenda√ß√£o)](#4-algoritmo-2-svd-sistema-de-recomenda√ß√£o)
5. [M√©tricas de Neg√≥cio](#5-m√©tricas-de-neg√≥cio-implementadas)
6. [Visualiza√ß√µes e UX](#6-visualiza√ß√µes-e-ux)
7. [Otimiza√ß√µes de Performance](#7-otimiza√ß√µes-de-performance)
8. [Valida√ß√£o e Qualidade](#8-valida√ß√£o-e-qualidade)
9. [Casos de Uso Atendidos](#9-casos-de-uso-atendidos)
10. [Diferenciais T√©cnicos](#10-diferenciais-t√©cnicos)
11. [Como Executar](#11-como-executar)

---

## 1. üìã **VIS√ÉO GERAL DA ARQUITETURA**

### **Stack Tecnol√≥gica Escolhida:**
```python
- Streamlit: Framework web para dashboards interativos
- Pandas/NumPy: Manipula√ß√£o e an√°lise de dados
- Plotly: Visualiza√ß√µes interativas avan√ßadas
- Scikit-learn: Algoritmos de Machine Learning
- Seaborn/Matplotlib: Visualiza√ß√µes estat√≠sticas complementares
```

### **Por que essas tecnologias?**
- **Streamlit**: Permite criar dashboards profissionais rapidamente, ideal para prototipagem e an√°lise de dados
- **Plotly**: Gr√°ficos interativos que permitem zoom, hover e filtros din√¢micos
- **Scikit-learn**: Biblioteca madura com algoritmos otimizados e bem documentados

### **Estrutura de Arquivos:**
```
Tradicional Bet/
‚îú‚îÄ‚îÄ streamlit_app.py              # Aplica√ß√£o principal
‚îú‚îÄ‚îÄ CLAUDE.md                     # Instru√ß√µes do projeto
‚îú‚îÄ‚îÄ DOCUMENTACAO_TECNICA_COMPLETA.md  # Este arquivo
‚îú‚îÄ‚îÄ public/
‚îÇ   ‚îî‚îÄ‚îÄ Logo.jpg                  # Logo da empresa
‚îî‚îÄ‚îÄ PROVA - AN√ÅLISE DE DADOS.xlsx # Dataset de exemplo
```

---

## 2. üèóÔ∏è **ARQUITETURA DO SISTEMA**

### **Padr√£o de Design Utilizado:**
```python
def main():
    # 1. Configura√ß√£o inicial e CSS
    setup_page_config()
    apply_custom_css()
    
    # 2. Interface de upload
    uploaded_file = st.file_uploader(...)
    
    # 3. Processamento e feature engineering
    df = load_data(uploaded_file)
    df_features = create_features(df)
    
    # 4. An√°lises e visualiza√ß√µes
    show_temporal_analysis(df)
    show_performance_analysis(df)
    
    # 5. Machine Learning (Anomalias + Recomenda√ß√µes)
    if show_anomalies:
        detect_anomalies(df)
    if show_recommendations:
        create_recommendation_system(df)
```

### **Feature Engineering Avan√ßado:**
```python
def create_features(df):
    # Features Temporais
    df['hora_do_dia'] = df['data'].dt.hour
    df['dia_da_semana'] = df['data'].dt.dayofweek
    df['dia_do_mes'] = df['data'].dt.day
    df['dia_semana_nome'] = df['dia_da_semana'].map({
        0: 'Segunda', 1: 'Ter√ßa', 2: 'Quarta', 3: 'Quinta',
        4: 'Sexta', 5: 'S√°bado', 6: 'Domingo'
    })
    
    # Per√≠odos do dia
    df['periodo_do_dia'] = pd.cut(df['hora_do_dia'], 
        bins=[0, 6, 12, 18, 24], 
        labels=['Madrugada', 'Manh√£', 'Tarde', 'Noite']
    )
    
    # Features Financeiras
    df['taxa_de_ganho'] = df['ganho'] / df['aposta']
    df['ggr'] = df['aposta'] - df['ganho']  # Gross Gaming Revenue
    
    # Features Comportamentais por Jogador
    df['frequencia_jogador'] = df.groupby('jogador_id')['aposta'].transform('count')
    df['ticket_medio_jogador'] = df.groupby('jogador_id')['aposta'].transform('mean')
    df['ggr_acumulado_jogador'] = df.groupby('jogador_id')['ggr'].transform('cumsum')
    
    return df
```

**Por que esse Feature Engineering?**
- **Temporais**: Identificar padr√µes sazonais, hor√°rios de pico, comportamento por dia da semana
- **Financeiras**: M√©tricas essenciais para cassinos (GGR √© o principal KPI da ind√∫stria)
- **Comportamentais**: Detectar perfis de jogadores (VIPs, casuais, problem√°ticos, fraudulentos)

---

## 3. ü§ñ **ALGORITMO 1: ISOLATION FOREST (Detec√ß√£o de Anomalias)**

### **Fundamento Matem√°tico:**

O Isolation Forest √© baseado no princ√≠pio de **isolamento**:
- **Hip√≥tese**: Anomalias s√£o mais f√°ceis de isolar que pontos normais
- **M√©todo**: Usa √°rvores de decis√£o bin√°rias aleat√≥rias para "isolar" pontos
- **Intui√ß√£o**: Pontos an√¥malos precisam de menos divis√µes para serem isolados

### **Como Funciona Internamente:**

```python
# 1. Constru√ß√£o das √Årvores (100 √°rvores por padr√£o)
def build_isolation_tree(data, max_depth):
    if len(data) <= 1 or max_depth <= 0:
        return LeafNode(size=len(data))
    
    # Seleciona feature aleat√≥ria
    feature = random.choice(features)
    
    # Seleciona ponto de corte aleat√≥rio entre min e max
    min_val, max_val = data[feature].min(), data[feature].max()
    split_value = random.uniform(min_val, max_val)
    
    # Divide os dados
    left_data = data[data[feature] < split_value]
    right_data = data[data[feature] >= split_value]
    
    return InternalNode(
        feature=feature,
        split_value=split_value,
        left=build_isolation_tree(left_data, max_depth-1),
        right=build_isolation_tree(right_data, max_depth-1)
    )

# 2. C√°lculo do Score de Anomalia
def isolation_score(point, tree):
    path_length = 0
    current_node = tree
    
    while not current_node.is_leaf():
        if point[current_node.feature] < current_node.split_value:
            current_node = current_node.left
        else:
            current_node = current_node.right
        path_length += 1
    
    # Adiciona estimativa para folha n√£o pura
    path_length += estimate_remaining_depth(current_node.size)
    return path_length
```

### **F√≥rmula do Score de Anomalia:**
```
s(x,n) = 2^(-E(h(x))/c(n))

Onde:
- E(h(x)) = profundidade m√©dia do ponto x nas √°rvores
- c(n) = comprimento m√©dio de busca em BST com n pontos
- c(n) = 2H(n-1) - (2(n-1)/n)
- H = n√∫mero harm√¥nico = ln(n) + Œ≥ (constante de Euler)
```

### **Interpreta√ß√£o dos Scores:**
- **Score ‚âà 1**: Anomalia clara (isolado rapidamente)
- **Score ‚âà 0.5**: Comportamento normal (profundidade m√©dia)
- **Score ‚âà 0**: Muito normal (dif√≠cil de isolar)

### **Por que escolher Isolation Forest?**

‚úÖ **Vantagens T√©cnicas:**
- **N√£o supervisionado**: N√£o precisa de labels de anomalias (ideally para detec√ß√£o de fraude)
- **Complexidade O(n log n)**: Escala bem com grandes datasets
- **Robusto a outliers**: N√£o afetado por distribui√ß√£o dos dados
- **Interpret√°vel**: Score de 0 a 1 f√°cil de explicar para business
- **Eficiente em mem√≥ria**: √Årvores pequenas (profundidade log n)

‚úÖ **Adequado para Cassinos:**
- **Detec√ß√£o autom√°tica**: Identifica comportamentos suspeitos sem supervis√£o
- **M√∫ltiplas dimens√µes**: Considera v√°rias features simultaneamente
- **Tempo real**: Pode avaliar novas transa√ß√µes instantaneamente
- **Flex√≠vel**: Funciona com diferentes tipos de fraude

### **Features Utilizadas para Anomalias:**
```python
anomaly_features = [
    'aposta',              # Valor apostado (detecta apostas muito altas/baixas)
    'ganho',               # Valor ganho (detecta ganhos suspeitos)
    'taxa_de_ganho',       # Ratio ganho/aposta (detecta padr√µes de vit√≥ria an√¥malos)
    'hora_do_dia',         # Hor√°rio (detecta atividade fora do hor√°rio normal)
    'frequencia_jogador',  # Frequ√™ncia de jogo (detecta bots ou v√≠cio)
    'ticket_medio_jogador' # Ticket m√©dio (detecta mudan√ßas de comportamento)
]

# Pr√©-processamento obrigat√≥rio
scaler = StandardScaler()
X_scaled = scaler.fit_transform(df[anomaly_features])

# Configura√ß√£o do modelo
isolation_forest = IsolationForest(
    n_estimators=100,        # 100 √°rvores para estabilidade
    contamination=0.05,      # 5% de anomalias esperadas
    random_state=42,         # Reprodutibilidade
    n_jobs=-1               # Paraleliza√ß√£o
)
```

### **M√©tricas de Qualidade do Modelo:**

1. **Separa√ß√£o de Scores**: 
   ```python
   score_separation = abs(mean_score_normal - mean_score_anomalous)
   # Ideal: > 0.3 (excelente separa√ß√£o entre classes)
   # 0.1-0.3: boa separa√ß√£o
   # < 0.1: modelo pode precisar ajuste
   ```

2. **Concentra√ß√£o de Anomalias**:
   ```python
   concentration = (num_anomalies / total_transactions) * 100
   # Ideal: 1-5% (balanceado, n√£o muitos falsos positivos)
   ```

3. **Qualidade do Threshold**:
   ```python
   threshold_quality = abs(threshold) * 10
   # Mede confian√ßa no ponto de corte
   # Maior = maior confian√ßa na separa√ß√£o
   ```

4. **Vari√¢ncia dos Scores**:
   ```python
   variance_normal = scores[normal_mask].var()
   variance_anomalous = scores[anomalous_mask].var()
   # Baixa vari√¢ncia = grupos bem definidos
   ```

### **Implementa√ß√£o da Detec√ß√£o:**

```python
def detect_anomalies(df, contamination=0.05):
    # 1. Prepara√ß√£o dos dados
    feature_cols = ['aposta', 'ganho', 'taxa_de_ganho', 'hora_do_dia']
    X = df[feature_cols].fillna(df[feature_cols].median())
    
    # 2. Normaliza√ß√£o (cr√≠tico para Isolation Forest)
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)
    
    # 3. Treinamento do modelo
    isolation_forest = IsolationForest(
        contamination=contamination,
        random_state=42,
        n_estimators=100
    )
    
    # 4. Predi√ß√£o
    anomaly_labels = isolation_forest.fit_predict(X_scaled)
    anomaly_scores = isolation_forest.score_samples(X_scaled)
    
    # 5. Adi√ß√£o dos resultados ao DataFrame
    df['anomalia'] = anomaly_labels  # 1 = normal, -1 = anomalia
    df['anomaly_score'] = anomaly_scores  # Score cont√≠nuo
    
    # 6. C√°lculo de estat√≠sticas do modelo
    model_stats = {
        'total_anomalias': sum(anomaly_labels == -1),
        'percentual_anomalias': (sum(anomaly_labels == -1) / len(df)) * 100,
        'threshold': isolation_forest.offset_,
        'features_utilizadas': feature_cols,
        'score_medio_normal': anomaly_scores[anomaly_labels == 1].mean(),
        'score_medio_anomalo': anomaly_scores[anomaly_labels == -1].mean()
    }
    
    return df, model_stats
```

---

## 4. üéØ **ALGORITMO 2: SVD (Sistema de Recomenda√ß√£o)**

### **Fundamento Matem√°tico:**

SVD (Singular Value Decomposition) decomp√µe a matriz de intera√ß√µes usu√°rio-item:

```
R = U √ó Œ£ √ó V^T

Onde:
- R: matriz usu√°rio-item original (jogadores √ó jogos)
- U: matriz de fatores latentes dos usu√°rios (n_users √ó k)
- Œ£: matriz diagonal com valores singulares (k √ó k)
- V^T: matriz de fatores latentes dos itens transposta (k √ó n_items)
- k: n√∫mero de componentes (dimens√µes latentes)
```

### **Interpreta√ß√£o dos Componentes:**

- **U**: Representa usu√°rios no espa√ßo de k dimens√µes latentes
- **V**: Representa itens no espa√ßo de k dimens√µes latentes  
- **Œ£**: Import√¢ncia de cada dimens√£o latente (ordenados decrescente)

**Exemplo conceitual das dimens√µes latentes:**
- Dimens√£o 1: Prefer√™ncia por "jogos de a√ß√£o" vs "jogos casuais"
- Dimens√£o 2: Prefer√™ncia por "jogos com jackpot" vs "jogos de mesa"
- Dimens√£o 3: Toler√¢ncia a "risco alto" vs "risco baixo"

### **Processo Detalhado de Implementa√ß√£o:**

```python
def create_recommendation_system(df):
    # 1. Constru√ß√£o da Matriz de Intera√ß√µes
    interactions = df.groupby(['jogador_id', 'jogo']).size().reset_index(name='num_jogadas')
    
    # 2. Cria√ß√£o do Sistema de Rating
    # Converte frequ√™ncia em rating de 1-5 usando log
    interactions['rating'] = np.minimum(5, 1 + np.log1p(interactions['num_jogadas']))
    
    # 3. Divis√£o Train/Test (80/20 por usu√°rio)
    train_interactions = []
    test_interactions = []
    
    for player_id in interactions['jogador_id'].unique():
        player_data = interactions[interactions['jogador_id'] == player_id]
        
        if len(player_data) < 3:  # Skip usu√°rios com poucas intera√ß√µes
            continue
            
        # Shuffle e divis√£o
        player_shuffled = player_data.sample(frac=1, random_state=42)
        n_train = max(1, int(0.8 * len(player_shuffled)))
        
        train_interactions.append(player_shuffled.iloc[:n_train])
        if len(player_shuffled) > n_train:
            test_interactions.append(player_shuffled.iloc[n_train:])
    
    train_df = pd.concat(train_interactions, ignore_index=True)
    
    # 4. Mapeamento de IDs para √çndices
    players = train_df['jogador_id'].unique()
    games = train_df['jogo'].unique()
    
    player_to_idx = {player: idx for idx, player in enumerate(players)}
    game_to_idx = {game: idx for idx, game in enumerate(games)}
    idx_to_game = {idx: game for game, idx in game_to_idx.items()}
    
    # 5. Constru√ß√£o da Matriz Esparsa
    n_players = len(players)
    n_games = len(games)
    
    row_indices = [player_to_idx[row['jogador_id']] for _, row in train_df.iterrows()]
    col_indices = [game_to_idx[row['jogo']] for _, row in train_df.iterrows()]
    ratings = train_df['rating'].values
    
    R = sp.csr_matrix((ratings, (row_indices, col_indices)), 
                      shape=(n_players, n_games))
    
    # 6. Aplica√ß√£o do SVD com Redu√ß√£o de Dimensionalidade
    n_components = min(30, min(n_players, n_games) - 1)
    svd_model = TruncatedSVD(n_components=n_components, random_state=42)
    
    # Decomposi√ß√£o: R ‚âà U_k √ó Œ£_k √ó V_k^T
    U_k = svd_model.fit_transform(R)  # n_players √ó k
    Vt_k = svd_model.components_      # k √ó n_games
    
    # 7. Reconstru√ß√£o da Matriz
    R_reconstructed = U_k @ Vt_k
    
    return {
        'model': svd_model,
        'U': U_k,
        'Vt': Vt_k,
        'R_original': R,
        'R_reconstructed': R_reconstructed,
        'player_to_idx': player_to_idx,
        'game_to_idx': game_to_idx,
        'idx_to_game': idx_to_game,
        'n_players': n_players,
        'n_games': n_games,
        'n_components': n_components,
        'variancia_explicada': svd_model.explained_variance_ratio_.sum(),
        'esparsidade': 1 - (R.nnz / (n_players * n_games))
    }
```

### **Algoritmo de Recomenda√ß√£o:**

```python
def get_recommendations_for_player(player_id, model_data, n_recommendations=5):
    if player_id not in model_data['player_to_idx']:
        return []  # Cold start - usu√°rio novo
    
    player_idx = model_data['player_to_idx'][player_id]
    
    # 1. Obter vetor do usu√°rio no espa√ßo latente
    user_vector = model_data['U'][player_idx]  # Vetor k-dimensional
    
    # 2. Calcular scores para todos os jogos
    game_scores = user_vector @ model_data['Vt']  # Produto escalar
    
    # 3. Remover jogos j√° jogados
    played_games = model_data['R_original'][player_idx].nonzero()[1]
    game_scores[played_games] = -np.inf
    
    # 4. Selecionar top N
    top_game_indices = np.argsort(game_scores)[::-1][:n_recommendations]
    
    # 5. Converter para nomes de jogos
    recommendations = []
    for game_idx in top_game_indices:
        if game_idx in model_data['idx_to_game']:
            game_name = model_data['idx_to_game'][game_idx]
            score = game_scores[game_idx]
            recommendations.append({
                'jogo': game_name,
                'score': score,
                'confidence': sigmoid(score)  # Normalizar para [0,1]
            })
    
    return recommendations

def sigmoid(x):
    """Converte scores em probabilidades"""
    return 1 / (1 + np.exp(-x))
```

### **Por que SVD para Recomenda√ß√µes?**

‚úÖ **Vantagens Matem√°ticas:**
- **Redu√ß√£o de Ru√≠do**: Mant√©m apenas os k componentes mais importantes
- **Captura de Padr√µes Latentes**: Identifica prefer√™ncias ocultas dos usu√°rios
- **Efici√™ncia Computacional**: Reduz dimensionalidade de milhares para dezenas
- **Fundamento S√≥lido**: Base matem√°tica robusta em √°lgebra linear
- **Generaliza√ß√£o**: Funciona bem mesmo com dados esparsos

‚úÖ **Adequado para Cassinos:**
- **Filtragem Colaborativa**: "Usu√°rios similares gostam de jogos similares"
- **Descoberta de Conte√∫do**: Recomenda jogos que o usu√°rio ainda n√£o conhece
- **Personaliza√ß√£o**: Cada usu√°rio recebe recomenda√ß√µes √∫nicas baseadas em seu perfil
- **Escalabilidade**: Funciona com milhares de jogadores e centenas de jogos
- **Interpretabilidade**: Dimens√µes latentes podem representar prefer√™ncias de jogo

### **Sistema de Rating Implementado:**
```python
# Convers√£o de intera√ß√µes em ratings
rating = min(5, 1 + log(1 + num_jogadas))

# Por que essa f√≥rmula logar√≠tmica?
# 1. Evita que usu√°rios muito ativos dominem o sistema
# 2. Escala [1,5] √© mais interpret√°vel que contagem bruta
# 3. Reduz impacto de outliers (usu√°rios com milhares de jogadas)
# 4. log(1 + x) √© suave e mon√≥tona crescente

# Exemplos:
# 1 jogada ‚Üí rating = 1 + log(2) ‚âà 1.69
# 5 jogadas ‚Üí rating = 1 + log(6) ‚âà 2.79  
# 20 jogadas ‚Üí rating = 1 + log(21) ‚âà 4.04
# 100 jogadas ‚Üí rating = 5 (m√°ximo)
```

### **M√©tricas de Qualidade do SVD:**

1. **Vari√¢ncia Explicada**:
   ```python
   variancia_explicada = sum(explained_variance_ratio_[:k])
   # Mede quanto da informa√ß√£o original foi preservada
   # Ideal: > 70% (boa representa√ß√£o)
   # < 50% indica que pode precisar mais componentes
   ```

2. **Esparsidade da Matriz**:
   ```python
   esparsidade = 1 - (interacoes_nao_zero / (n_players * n_games))
   # Mede o qu√£o "vazia" √© a matriz original
   # Cassinos: geralmente > 99% (muito esparsa)
   # SVD funciona bem mesmo com alta esparsidade
   ```

3. **Precision@K e Recall@K**:
   ```python
   def evaluate_recommendations(test_data, model, k=5):
       precisions, recalls = [], []
       
       for user_id in test_data['jogador_id'].unique():
           # Jogos que o usu√°rio realmente gostou no teste
           actual_games = set(test_data[test_data['jogador_id'] == user_id]['jogo'])
           
           # Jogos recomendados pelo modelo
           recommended_games = set([rec['jogo'] for rec in 
                                  get_recommendations_for_player(user_id, model, k)])
           
           # Jogos corretos (interse√ß√£o)
           relevant_recommended = len(actual_games & recommended_games)
           
           # Precision@K: dos K recomendados, quantos s√£o relevantes?
           precision = relevant_recommended / k if k > 0 else 0
           
           # Recall@K: dos relevantes, quantos foram recomendados?
           recall = relevant_recommended / len(actual_games) if len(actual_games) > 0 else 0
           
           precisions.append(precision)
           recalls.append(recall)
       
       return np.mean(precisions), np.mean(recalls)
   ```

4. **MAE e RMSE (Mean Absolute Error e Root Mean Square Error)**:
   ```python
   def calculate_rating_errors(test_data, model):
       errors = []
       squared_errors = []
       
       for _, row in test_data.iterrows():
           user_id = row['jogador_id']
           game = row['jogo']
           actual_rating = row['rating']
           
           # Predizer rating
           predicted_rating = predict_rating(user_id, game, model)
           
           error = abs(actual_rating - predicted_rating)
           squared_error = (actual_rating - predicted_rating) ** 2
           
           errors.append(error)
           squared_errors.append(squared_error)
       
       mae = np.mean(errors)
       rmse = np.sqrt(np.mean(squared_errors))
       
       return mae, rmse
   ```

5. **Cobertura do Cat√°logo**:
   ```python
   coverage = len(games_recommended) / total_games
   # Mede diversidade: quantos % dos jogos s√£o recomendados
   # Alta cobertura = sistema explora bem o cat√°logo
   ```

6. **An√°lise de Cold Start**:
   ```python
   # Usu√°rios com poucas intera√ß√µes (‚â§3)
   cold_start_users = users_with_few_interactions.count()
   cold_start_ratio = cold_start_users / total_users
   
   # Problema: SVD n√£o funciona bem para usu√°rios novos
   # Solu√ß√£o: usar recomenda√ß√µes populares para cold start
   ```

---

## 5. üìä **M√âTRICAS DE NEG√ìCIO IMPLEMENTADAS**

### **KPIs Financeiros (Essenciais para Cassinos):**

```python
# 1. GGR (Gross Gaming Revenue) - M√©trica #1 da ind√∫stria
def calculate_ggr(df):
    return df['aposta'].sum() - df['ganho'].sum()

# 2. Margem do Cassino (House Edge Efetiva)
def calculate_margin(df):
    total_bets = df['aposta'].sum()
    total_wins = df['ganho'].sum()
    return ((total_bets - total_wins) / total_bets) * 100

# 3. Ticket M√©dio (Average Bet Size)
def calculate_avg_ticket(df):
    return df['aposta'].mean()

# 4. RTP (Return to Player) - Regulamenta√ß√£o
def calculate_rtp(df):
    return (df['ganho'].sum() / df['aposta'].sum()) * 100

# 5. LTV (Customer Lifetime Value)
def calculate_ltv(df):
    player_metrics = df.groupby('jogador_id').agg({
        'ggr': 'sum',
        'data': lambda x: (x.max() - x.min()).days + 1
    })
    return player_metrics['ggr'] / player_metrics['data']
```

### **KPIs Operacionais:**

```python
# 6. Taxa de Reten√ß√£o (cr√≠tico para crescimento)
def calculate_retention(df):
    # Jogadores que voltaram ap√≥s primeiro dia
    first_play = df.groupby('jogador_id')['data'].min()
    last_play = df.groupby('jogador_id')['data'].max()
    
    retained = ((last_play - first_play).dt.days > 0).sum()
    total = len(first_play)
    
    return (retained / total) * 100

# 7. Frequ√™ncia M√©dia de Jogo
def calculate_avg_frequency(df):
    return df.groupby('jogador_id').size().mean()

# 8. Diversidade de Cat√°logo (Portfolio Health)
def calculate_catalog_diversity(df):
    games_played = df['jogo'].nunique()
    total_games = df['jogo'].nunique()  # Assumindo que temos todo o cat√°logo
    return (games_played / total_games) * 100

# 9. Concentra√ß√£o por Provider
def calculate_provider_concentration(df):
    provider_revenue = df.groupby('fornecedor')['ggr'].sum()
    total_revenue = provider_revenue.sum()
    
    # √çndice Herfindahl-Hirschman (concentra√ß√£o de mercado)
    hhi = ((provider_revenue / total_revenue) ** 2).sum()
    return hhi  # 0 = diversificado, 1 = monopolizado

# 10. Taxa de Convers√£o (Depositors ‚Üí Players)
def calculate_conversion_rate(df):
    # Assumindo que temos dados de dep√≥sito
    depositors = df['jogador_id'].nunique()
    active_players = df[df['aposta'] > 0]['jogador_id'].nunique()
    return (active_players / depositors) * 100
```

### **Segmenta√ß√£o de Jogadores (RFM Analysis Adaptado):**

```python
def segment_players(df):
    # Rec√™ncia, Frequ√™ncia, Monet√°rio
    player_metrics = df.groupby('jogador_id').agg({
        'data': 'max',          # √öltima jogada (Rec√™ncia)
        'aposta': ['count', 'sum'],  # Frequ√™ncia e Volume Monet√°rio
        'ggr': 'sum'            # Valor para o cassino
    }).round(2)
    
    # Calcular rec√™ncia em dias
    max_date = df['data'].max()
    player_metrics['recencia'] = (max_date - player_metrics[('data', 'max')]).dt.days
    
    # Definir quartis para segmenta√ß√£o
    player_metrics['R_score'] = pd.qcut(player_metrics['recencia'], 4, labels=[4,3,2,1])
    player_metrics['F_score'] = pd.qcut(player_metrics[('aposta', 'count')], 4, labels=[1,2,3,4])
    player_metrics['M_score'] = pd.qcut(player_metrics[('ggr', 'sum')], 4, labels=[1,2,3,4])
    
    # Combinar scores em segmentos
    player_metrics['RFM_Score'] = (
        player_metrics['R_score'].astype(str) +
        player_metrics['F_score'].astype(str) +
        player_metrics['M_score'].astype(str)
    )
    
    # Definir segmentos de neg√≥cio
    def categorize_player(rfm_score):
        if rfm_score in ['444', '443', '434', '344']:
            return 'VIP'
        elif rfm_score[0] in ['4', '3'] and rfm_score[1:] in ['44', '43', '34']:
            return 'Leal'
        elif rfm_score[0] in ['2', '1']:
            return 'Em Risco'
        elif rfm_score[1] == '4':
            return 'Frequente'
        else:
            return 'Casual'
    
    player_metrics['Segmento'] = player_metrics['RFM_Score'].apply(categorize_player)
    
    return player_metrics
```

---

## 6. üé® **VISUALIZA√á√ïES E UX**

### **Design System Implementado:**

```css
/* Paleta de Cores Profissional */
:root {
    --primary-blue: #3b82f6;
    --primary-purple: #8b5cf6;
    --dark-blue: #1e40af;
    --gradient-main: linear-gradient(135deg, #0D1528 0%, #1e3a8a 100%);
    --gradient-accent: linear-gradient(90deg, #3b82f6, #8b5cf6);
    --success-green: #10b981;
    --warning-amber: #f59e0b;
    --danger-red: #ef4444;
}

/* Tipografia Moderna */
font-family: 'Inter', -apple-system, BlinkMacSystemFont, sans-serif;

/* Eleva√ß√£o e Sombras (Material Design) */
box-shadow: 
    0 4px 6px -1px rgba(0, 0, 0, 0.1),
    0 2px 4px -1px rgba(0, 0, 0, 0.06);
```

### **Tipos de Visualiza√ß√£o por Caso de Uso:**

1. **KPIs Principais**: 
   ```python
   st.metric(
       label="üí∞ GGR Total",
       value="R$ 1.234.567,89",
       delta="‚ÜóÔ∏è +12.5%",
       help="Gross Gaming Revenue - receita bruta do cassino"
   )
   ```

2. **Evolu√ß√£o Temporal**:
   ```python
   fig = go.Scatter(
       x=dates, y=values,
       mode='lines+markers',
       fill='tonexty',  # √Årea preenchida
       line=dict(color='#3b82f6', width=3),
       marker=dict(size=6, color='#3b82f6'),
       fillcolor='rgba(59, 130, 246, 0.2)'
   )
   ```

3. **Compara√ß√µes e Rankings**:
   ```python
   fig = px.bar(
       data, x='values', y='categories',
       orientation='h',  # Horizontal para melhor legibilidade
       color='values',
       color_continuous_scale='Blues',
       text_auto=True
   )
   ```

4. **Distribui√ß√µes (Ridgeline Plots)**:
   ```python
   # Substituindo boxplots tradicionais por ridgeline horizontais
   for i, category in enumerate(categories):
       fig.add_trace(go.Violin(
           x=data[data['category'] == category]['values'],
           y=[category] * len(data),
           orientation='h',  # Horizontal
           side='positive',
           fillcolor=colors[i],
           opacity=0.7,
           showlegend=False
       ))
   ```

5. **Correla√ß√µes e Heatmaps**:
   ```python
   fig = px.imshow(
       correlation_matrix,
       color_continuous_scale='RdBu',
       aspect='auto',
       title="Matriz de Correla√ß√£o"
   )
   # Texto customizado com formata√ß√£o brasileira
   fig.update_traces(text=formatted_text_matrix, texttemplate="%{text}")
   ```

6. **Machine Learning Visualizations**:
   ```python
   # Scatter 3D para an√°lise de anomalias
   fig = go.Figure(data=[go.Scatter3d(
       x=features[:, 0], y=features[:, 1], z=features[:, 2],
       mode='markers',
       marker=dict(
           size=8,
           color=anomaly_scores,
           colorscale='Viridis',
           opacity=0.8,
           colorbar=dict(title="Anomaly Score")
       )
   )])
   ```

### **Responsividade e Adapta√ß√£o:**

```python
# Layout adaptativo baseado em colunas
col1, col2, col3, col4 = st.columns([2, 1, 1, 1])

# Configura√ß√£o responsiva para gr√°ficos
fig.update_layout(
    autosize=True,
    height=600,
    margin=dict(l=0, r=0, t=50, b=0)
)

st.plotly_chart(fig, use_container_width=True)
```

### **Microintera√ß√µes e Feedback:**

```css
/* Hover Effects */
.stButton > button:hover {
    transform: translateY(-2px);
    box-shadow: 0 8px 25px rgba(59, 130, 246, 0.4);
    transition: all 0.3s ease;
}

/* Loading States */
.stSpinner {
    color: var(--primary-blue);
}

/* Status Indicators */
.success-indicator { color: var(--success-green); }
.warning-indicator { color: var(--warning-amber); }
.danger-indicator { color: var(--danger-red); }
```

---

## 7. üîß **OTIMIZA√á√ïES DE PERFORMANCE**

### **1. Caching Estrat√©gico:**

```python
# Cache de dados persistente
@st.cache_data(show_spinner=False, ttl=3600)  # 1 hora
def load_data(uploaded_file):
    """Cache do carregamento - evita reprocessar arquivo"""
    return pd.read_excel(uploaded_file)

# Cache de feature engineering
@st.cache_data(show_spinner=False, ttl=1800)  # 30 minutos
def create_features(df):
    """Cache das features engineeradas"""
    return enhanced_df

# Cache de modelos ML (mais custosos)
@st.cache_resource(show_spinner=False, ttl=7200)  # 2 horas
def train_isolation_forest(X_scaled):
    """Cache do modelo treinado"""
    return model.fit(X_scaled)
```

### **2. Lazy Loading e Processamento Condicional:**

```python
# ML s√≥ executa quando o usu√°rio solicita
if show_anomalies and uploaded_file:
    with st.spinner("ü§ñ Treinando Isolation Forest..."):
        df_with_anomalies, model_stats = detect_anomalies(df)
else:
    st.info("üí° Ative a detec√ß√£o de anomalias para an√°lise ML")

# An√°lises pesadas s√≥ quando necess√°rio
if st.button("üîÑ Executar An√°lise Completa"):
    # Processamento intensivo aqui
    pass
```

### **3. Otimiza√ß√£o de DataFrames:**

```python
# Tipos de dados eficientes
def optimize_dtypes(df):
    # Categoricals para strings repetitivas
    df['fornecedor'] = df['fornecedor'].astype('category')
    df['jogo'] = df['jogo'].astype('category')
    
    # Downcast num√©ricos
    df['aposta'] = pd.to_numeric(df['aposta'], downcast='float')
    df['jogador_id'] = pd.to_numeric(df['jogador_id'], downcast='integer')
    
    return df

# Processamento vetorizado
df['ggr'] = df['aposta'] - df['ganho']  # Ao inv√©s de apply()
```

### **4. Formata√ß√£o Otimizada:**

```python
# Fun√ß√£o reutiliz√°vel para formata√ß√£o brasileira
@st.cache_data
def format_brazilian_number(value, decimals=0):
    """Cache da formata√ß√£o para evitar rec√°lculo"""
    if pd.isna(value): 
        return "0"
    
    formatted = f"{value:,.{decimals}f}"
    # Convers√£o para padr√£o brasileiro
    return formatted.replace(',', 'TEMP').replace('.', ',').replace('TEMP', '.')

# Fun√ß√µes auxiliares globais
def format_currency_br(val):
    return f"R$ {format_brazilian_number(val, 2)}"

def format_percentage_br(val):
    return f"{format_brazilian_number(val, 2)}%"
```

### **5. Configura√ß√£o de Plotly Otimizada:**

```python
# Template global para evitar repeti√ß√£o
import plotly.io as pio

# Configura√ß√£o brasileira padr√£o
pio.templates["brazilian"] = go.layout.Template(
    layout=go.Layout(
        separators=",.",  # V√≠rgula decimal, ponto milhar
        hovermode="closest",
        font=dict(family="Inter, sans-serif"),
        colorway=['#3b82f6', '#10b981', '#f59e0b', '#ef4444']
    )
)
pio.templates.default = "plotly_white+brazilian"
```

### **6. Gest√£o de Mem√≥ria:**

```python
# Limpeza expl√≠cita de vari√°veis grandes
def process_large_dataset(df):
    # Processamento
    result = expensive_operation(df)
    
    # Limpeza
    del df  # Remove refer√™ncia
    gc.collect()  # For√ßa garbage collection
    
    return result

# Processamento em chunks para datasets muito grandes
def process_in_chunks(df, chunk_size=10000):
    results = []
    for chunk in pd.read_csv(file, chunksize=chunk_size):
        chunk_result = process_chunk(chunk)
        results.append(chunk_result)
    
    return pd.concat(results, ignore_index=True)
```

---

## 8. üß™ **VALIDA√á√ÉO E QUALIDADE**

### **1. Validation Split Adequado para Recomenda√ß√µes:**

```python
def create_temporal_split(interactions_df, test_ratio=0.2):
    """
    Split temporal por usu√°rio - mais realista que split aleat√≥rio
    """
    train_interactions = []
    test_interactions = []
    
    for user_id in interactions_df['jogador_id'].unique():
        user_data = interactions_df[interactions_df['jogador_id'] == user_id]
        
        # Requisito m√≠nimo de intera√ß√µes
        if len(user_data) < 3:
            continue
        
        # Ordena√ß√£o temporal
        user_data = user_data.sort_values('data')
        
        # Split 80/20 preservando ordem temporal
        n_train = max(1, int((1 - test_ratio) * len(user_data)))
        
        train_data = user_data.iloc[:n_train]
        test_data = user_data.iloc[n_train:]
        
        train_interactions.append(train_data)
        if len(test_data) > 0:
            test_interactions.append(test_data)
    
    return pd.concat(train_interactions), pd.concat(test_interactions)
```

**Por que split temporal por usu√°rio?**
- **Evita data leakage**: N√£o usa informa√ß√£o do futuro para predizer o passado
- **Simula cen√°rio real**: Usu√°rio com hist√≥rico limitado querendo recomenda√ß√µes
- **Avalia cold start**: Como o modelo se comporta com pouco hist√≥rico
- **Mant√©m distribui√ß√£o**: Cada usu√°rio mant√©m propor√ß√£o de seus dados

### **2. M√©tricas de Qualidade Implementadas:**

#### **Para Detec√ß√£o de Anomalias:**

```python
def evaluate_anomaly_detection(df, model, threshold):
    """M√©tricas espec√≠ficas para avaliar qualidade da detec√ß√£o"""
    
    # 1. Separa√ß√£o de Classes
    normal_scores = df[df['anomalia'] == 1]['anomaly_score']
    anomaly_scores = df[df['anomalia'] == -1]['anomaly_score']
    
    separation = abs(normal_scores.mean() - anomaly_scores.mean())
    
    # 2. Silhouette Score (coes√£o intra-classe vs separa√ß√£o inter-classe)
    from sklearn.metrics import silhouette_score
    
    X_scaled = model.transform(df[feature_columns])
    silhouette_avg = silhouette_score(X_scaled, df['anomalia'])
    
    # 3. Concentra√ß√£o de Anomalias
    anomaly_rate = (df['anomalia'] == -1).mean()
    
    # 4. Estabilidade do Threshold
    threshold_stability = 1 / (1 + abs(threshold))  # Normalizado
    
    return {
        'separation_score': separation,
        'silhouette_score': silhouette_avg,
        'anomaly_concentration': anomaly_rate,
        'threshold_stability': threshold_stability,
        'overall_quality': np.mean([separation, silhouette_avg, threshold_stability])
    }
```

#### **Para Sistema de Recomenda√ß√£o:**

```python
def comprehensive_recommendation_evaluation(train_data, test_data, model):
    """Avalia√ß√£o completa do sistema de recomenda√ß√£o"""
    
    # 1. M√©tricas de Accuracy
    precision_5 = calculate_precision_at_k(test_data, model, k=5)
    recall_5 = calculate_recall_at_k(test_data, model, k=5)
    f1_5 = 2 * (precision_5 * recall_5) / (precision_5 + recall_5)
    
    # 2. M√©tricas de Rating Prediction
    mae, rmse = calculate_rating_errors(test_data, model)
    
    # 3. Cobertura e Diversidade
    catalog_coverage = calculate_catalog_coverage(model)
    diversity_score = calculate_recommendation_diversity(model)
    
    # 4. Cold Start Performance
    cold_start_precision = evaluate_cold_start_users(test_data, model)
    
    # 5. Novelty (recomenda itens pouco populares?)
    novelty_score = calculate_novelty(model, train_data)
    
    # 6. Fairness (distribui recomenda√ß√µes equitativamente?)
    fairness_score = calculate_recommendation_fairness(model)
    
    return {
        'accuracy': {
            'precision@5': precision_5,
            'recall@5': recall_5,
            'f1@5': f1_5,
            'mae': mae,
            'rmse': rmse
        },
        'coverage': {
            'catalog_coverage': catalog_coverage,
            'diversity': diversity_score
        },
        'robustness': {
            'cold_start_precision': cold_start_precision,
            'novelty': novelty_score,
            'fairness': fairness_score
        }
    }

def calculate_novelty(model, train_data):
    """Mede se o modelo recomenda itens pouco populares (boa descoberta)"""
    item_popularity = train_data.groupby('jogo').size()
    item_popularity_norm = item_popularity / item_popularity.sum()
    
    # Novelty = m√©dia da raridade dos itens recomendados
    novelty_scores = []
    for user in train_data['jogador_id'].unique():
        recommendations = get_recommendations_for_player(user, model, 10)
        if recommendations:
            user_novelty = np.mean([
                -np.log2(item_popularity_norm.get(rec['jogo'], 1e-6))
                for rec in recommendations
            ])
            novelty_scores.append(user_novelty)
    
    return np.mean(novelty_scores) if novelty_scores else 0
```

### **3. Testes de Sanidade (Sanity Checks):**

```python
def run_sanity_checks(df, model_results):
    """Testes b√°sicos para garantir qualidade dos dados e modelos"""
    
    checks = {
        'data_quality': {
            'no_null_in_critical': df[['aposta', 'ganho', 'jogador_id']].isnull().sum().sum() == 0,
            'positive_bets': (df['aposta'] > 0).all(),
            'valid_dates': df['data'].notna().all(),
            'reasonable_values': df['aposta'].between(0.01, 100000).all()
        },
        'business_logic': {
            'ggr_calculation': abs(df['ggr'].sum() - (df['aposta'].sum() - df['ganho'].sum())) < 0.01,
            'rtp_reasonable': 0.70 <= (df['ganho'].sum() / df['aposta'].sum()) <= 0.99,
            'players_consistency': df['jogador_id'].nunique() > 0
        },
        'model_quality': {
            'anomaly_rate_reasonable': 0.001 <= model_results.get('anomaly_rate', 0) <= 0.1,
            'recommendations_diverse': len(set([r['jogo'] for r in model_results.get('sample_recs', [])])) > 1
        }
    }
    
    # Log dos resultados
    for category, tests in checks.items():
        failed_tests = [test for test, passed in tests.items() if not passed]
        if failed_tests:
            st.warning(f"‚ö†Ô∏è {category}: Testes falharam: {failed_tests}")
        else:
            st.success(f"‚úÖ {category}: Todos os testes passaram")
    
    return checks
```

### **4. Monitoramento de Drift:**

```python
def monitor_data_drift(historical_df, current_df):
    """Detecta mudan√ßas na distribui√ß√£o dos dados ao longo do tempo"""
    
    from scipy.stats import ks_2samp
    
    drift_results = {}
    
    numerical_columns = ['aposta', 'ganho', 'taxa_de_ganho']
    
    for column in numerical_columns:
        # Kolmogorov-Smirnov test para detectar mudan√ßa de distribui√ß√£o
        ks_stat, p_value = ks_2samp(
            historical_df[column].dropna(),
            current_df[column].dropna()
        )
        
        drift_results[column] = {
            'ks_statistic': ks_stat,
            'p_value': p_value,
            'drift_detected': p_value < 0.05,  # Signific√¢ncia 5%
            'drift_severity': 'Alto' if ks_stat > 0.1 else 'Baixo'
        }
    
    return drift_results
```

---

## 9. üéØ **CASOS DE USO ATENDIDOS**

### **1. Gest√£o Operacional Di√°ria:**

**Persona**: Gerente de Opera√ß√µes
**Necessidades**:
- Monitorar KPIs em tempo real
- Identificar problemas rapidamente
- Relat√≥rios executivos automatizados

**Solu√ß√µes Implementadas**:
```python
# Dashboard principal com KPIs cr√≠ticos
main_kpis = {
    'GGR Total': format_currency_br(ggr_total),
    'Margem %': format_percentage_br(margin),
    'Jogadores Ativos': format_number(active_players),
    'Ticket M√©dio': format_currency_br(avg_ticket)
}

# Alertas autom√°ticos
if margin < 5:
    st.error("üö® ALERTA: Margem abaixo do esperado!")
if anomaly_rate > 10:
    st.warning("‚ö†Ô∏è Alto volume de transa√ß√µes suspeitas detectadas")
```

### **2. Detec√ß√£o de Fraude e Compliance:**

**Persona**: Analista de Risco/Compliance
**Necessidades**:
- Identificar comportamentos suspeitos
- Gerar relat√≥rios para reguladores
- Monitorar jogadores problema

**Solu√ß√µes Implementadas**:
```python
# Sistema ML para detec√ß√£o autom√°tica
suspicious_players = df[df['anomalia'] == -1]['jogador_id'].unique()

# Exporta√ß√£o para investiga√ß√£o
export_suspicious_transactions = df[
    (df['anomalia'] == -1) | 
    (df['aposta'] > df['aposta'].quantile(0.99)) |
    (df['taxa_de_ganho'] > 2.0)  # Ganhos suspeitos
][['jogador_id', 'data', 'aposta', 'ganho', 'anomaly_score']]

# Classifica√ß√£o de risco
risk_categories = {
    'Alto Risco': df['anomaly_score'] < -0.5,
    'M√©dio Risco': df['anomaly_score'].between(-0.5, -0.2),
    'Baixo Risco': df['anomaly_score'] > -0.2
}
```

### **3. Estrat√©gia de Produto e Portf√≥lio:**

**Persona**: Product Manager
**Necessidades**:
- Analisar performance de jogos
- Identificar gaps no portf√≥lio
- Otimizar mix de produtos

**Solu√ß√µes Implementadas**:
```python
# An√°lise de performance por jogo
game_performance = df.groupby('jogo').agg({
    'ggr': 'sum',
    'aposta': 'count',
    'jogador_id': 'nunique'
}).rename(columns={'aposta': 'transacoes', 'jogador_id': 'jogadores_unicos'})

game_performance['ggr_per_player'] = game_performance['ggr'] / game_performance['jogadores_unicos']
game_performance['engagement'] = game_performance['transacoes'] / game_performance['jogadores_unicos']

# Classifica√ß√£o de jogos
def classify_game(row):
    if row['ggr'] > game_performance['ggr'].quantile(0.8):
        return 'Star' if row['jogadores_unicos'] > game_performance['jogadores_unicos'].quantile(0.8) else 'Cash Cow'
    else:
        return 'Question Mark' if row['jogadores_unicos'] > game_performance['jogadores_unicos'].quantile(0.5) else 'Dog'

game_performance['categoria_bcg'] = game_performance.apply(classify_game, axis=1)

# Recomenda√ß√µes estrat√©gicas
underperforming_games = game_performance[game_performance['categoria_bcg'] == 'Dog']
st.warning(f"üí° {len(underperforming_games)} jogos com baixa performance identificados para revis√£o")
```

### **4. CRM e Marketing Personalizado:**

**Persona**: Gerente de Marketing/CRM
**Necessidades**:
- Segmentar jogadores por comportamento
- Campanhas direcionadas
- Identificar oportunidades de up-sell

**Solu√ß√µes Implementadas**:
```python
# Segmenta√ß√£o RFM avan√ßada
player_segments = segment_players(df)

# Estrat√©gias por segmento
segment_strategies = {
    'VIP': {
        'action': 'Reten√ß√£o premium',
        'games_to_recommend': get_high_value_games(),
        'communication': 'Gerente dedicado',
        'offers': 'B√¥nus exclusivos'
    },
    'Em Risco': {
        'action': 'Campanha de reativa√ß√£o',
        'games_to_recommend': get_popular_games(),
        'communication': 'Email + SMS',
        'offers': 'Free spins'
    },
    'Novo': {
        'action': 'Onboarding',
        'games_to_recommend': get_beginner_friendly_games(),
        'communication': 'Tutorial + suporte',
        'offers': 'B√¥nus de boas-vindas'
    }
}

# Recomenda√ß√µes personalizadas por ML
for player_id in active_players:
    recommendations = get_recommendations_for_player(player_id, recommendation_model, n=5)
    segment = player_segments.loc[player_id, 'Segmento']
    
    # Campanha personalizada
    personalized_campaign = create_campaign(player_id, segment, recommendations)
```

### **5. An√°lise Financeira e Forecasting:**

**Persona**: CFO/Controller
**Necessidades**:
- Previs√£o de receita
- An√°lise de rentabilidade
- Budget e planejamento

**Solu√ß√µes Implementadas**:
```python
# An√°lise de tend√™ncias temporais
monthly_trends = df.groupby(df['data'].dt.to_period('M')).agg({
    'ggr': 'sum',
    'aposta': 'sum',
    'jogador_id': 'nunique'
})

# Proje√ß√£o simples baseada em tend√™ncia
from sklearn.linear_model import LinearRegression

# Preparar dados para previs√£o
X = np.arange(len(monthly_trends)).reshape(-1, 1)
y = monthly_trends['ggr'].values

model = LinearRegression().fit(X, y)

# Pr√≥ximos 3 meses
next_months = np.arange(len(monthly_trends), len(monthly_trends) + 3).reshape(-1, 1)
forecast = model.predict(next_months)

# M√©tricas financeiras
metrics = {
    'Receita Projetada (3M)': forecast.sum(),
    'ARPU (Average Revenue Per User)': monthly_trends['ggr'].iloc[-1] / monthly_trends['jogador_id'].iloc[-1],
    'Growth Rate': ((monthly_trends['ggr'].iloc[-1] / monthly_trends['ggr'].iloc[0]) ** (1/len(monthly_trends)) - 1) * 100,
    'Player LTV': player_segments['ggr_total'].mean()
}
```

---

## 10. üí° **DIFERENCIAIS T√âCNICOS**

### **1. Formata√ß√£o Brasileira Completa:**

**Problema**: Streamlit e Plotly usam formata√ß√£o americana por padr√£o
**Solu√ß√£o**: Sistema robusto de formata√ß√£o brasileira

```python
# Configura√ß√£o de locale + fun√ß√£o customizada + templates Plotly
import locale
try:
    locale.setlocale(locale.LC_ALL, 'pt_BR.UTF-8')
except:
    pass  # Fallback

def format_brazilian_number(value, decimals=0):
    formatted = f"{value:,.{decimals}f}"
    return formatted.replace(',', 'TEMP').replace('.', ',').replace('TEMP', '.')

# Template global Plotly
pio.templates["brazilian"] = go.layout.Template(
    layout=go.Layout(separators=",.")  # V√≠rgula decimal, ponto milhar
)

# Aplica√ß√£o em DataFrames
df.style.format({
    'ggr_total': lambda x: f"R$ {format_brazilian_number(x, 2)}",
    'percentual': lambda x: f"{format_brazilian_number(x, 1)}%"
})
```

### **2. Ridgeline Plots Horizontais:**

**Problema**: Boxplots tradicionais n√£o eram claros para visualizar distribui√ß√µes
**Solu√ß√£o**: Implementa√ß√£o de ridgeline plots usando violin plots rotacionados

```python
def create_horizontal_ridgeline(data, categories, values):
    fig = go.Figure()
    
    for i, category in enumerate(categories):
        category_data = data[data[category_col] == category][value_col]
        
        fig.add_trace(go.Violin(
            x=category_data,  # Valores no eixo X
            y=[category] * len(category_data),  # Categoria repetida no Y
            orientation='h',  # Horizontal
            side='positive',
            width=0.8,
            fill='tonexty',
            fillcolor=f'rgba({colors[i]}, 0.3)',
            line_color=colors[i],
            showlegend=False
        ))
    
    fig.update_layout(
        yaxis=dict(categoryorder='array', categoryarray=categories[::-1])
    )
    
    return fig
```

**Vantagens**:
- Melhor compara√ß√£o entre categorias
- Visualiza√ß√£o mais limpa das distribui√ß√µes
- Espa√ßo mais eficiente horizontalmente

### **3. Pipeline ML Integrado:**

**Diferencial**: Anomalias + Recomenda√ß√µes em interface unificada

```python
# Execu√ß√£o condicional e paralela quando poss√≠vel
if show_anomalies and show_recommendations:
    with st.spinner("ü§ñ Executando ML Pipeline..."):
        # Executa ambos algoritmos e mostra progresso
        anomaly_results = detect_anomalies(df)
        recommendation_results = create_recommendation_system(df)
        
        # Combina insights
        combined_insights = cross_reference_ml_results(anomaly_results, recommendation_results)
```

**Insights Cruzados**:
- Jogadores an√¥malos que tamb√©m t√™m padr√µes de recomenda√ß√£o √∫nicos
- Jogos frequentemente recomendados mas com alta taxa de anomalias
- Correla√ß√£o entre comportamento suspeito e prefer√™ncias de jogo

### **4. Export Inteligente:**

**Problema**: Usu√°rios precisam de dados actionables, n√£o apenas visualiza√ß√µes
**Solu√ß√£o**: CSV com dados processados + insights autom√°ticos

```python
def create_intelligent_export(df, ml_results):
    export_data = []
    
    # Para cada jogador an√¥malo
    for player_id in anomalous_players:
        player_data = df[df['jogador_id'] == player_id]
        recommendations = get_recommendations_for_player(player_id, model)
        
        export_row = {
            'jogador_id': player_id,
            'risk_score': ml_results['anomaly_scores'][player_id],
            'total_apostado': player_data['aposta'].sum(),
            'ggr_gerado': player_data['ggr'].sum(),
            'frequencia_jogos': len(player_data),
            'ultimo_jogo': player_data['data'].max(),
            'jogos_recomendados': ', '.join([r['jogo'] for r in recommendations[:3]]),
            'motivo_suspeita': classify_suspicion_reason(player_data, ml_results),
            'acao_sugerida': suggest_action(player_data, ml_results),
            'prioridade': calculate_priority(player_data, ml_results)
        }
        export_data.append(export_row)
    
    return pd.DataFrame(export_data)

def classify_suspicion_reason(player_data, ml_results):
    """Explica por que o jogador foi classificado como an√¥malo"""
    reasons = []
    
    if player_data['aposta'].max() > threshold_high_bet:
        reasons.append("Apostas muito altas")
    if player_data['taxa_de_ganho'].mean() > threshold_win_rate:
        reasons.append("Taxa de vit√≥ria suspeita")
    if player_data.groupby(player_data['data'].dt.date).size().max() > threshold_frequency:
        reasons.append("Frequ√™ncia anormal")
        
    return " | ".join(reasons) if reasons else "Padr√£o comportamental at√≠pico"
```

### **5. An√°lise de Qualidade em Tempo Real:**

**Diferencial**: M√©tricas de qualidade dos modelos ML s√£o calculadas e exibidas automaticamente

```python
# Avalia√ß√£o autom√°tica da qualidade do modelo
def display_model_quality_metrics(model, data, predictions):
    col1, col2, col3 = st.columns(3)
    
    with col1:
        # Separa√ß√£o de classes
        separation = calculate_class_separation(predictions)
        if separation > 0.3:
            st.success(f"‚úÖ Separa√ß√£o: {separation:.3f}")
        else:
            st.warning(f"‚ö†Ô∏è Separa√ß√£o: {separation:.3f}")
    
    with col2:
        # Estabilidade
        stability = calculate_model_stability(model)
        st.metric("üéöÔ∏è Estabilidade", f"{stability:.3f}")
    
    with col3:
        # Confian√ßa
        confidence = calculate_prediction_confidence(predictions)
        st.metric("üéØ Confian√ßa", f"{confidence:.3f}")

# Recomenda√ß√µes autom√°ticas para melhorar modelo
def suggest_model_improvements(quality_metrics):
    suggestions = []
    
    if quality_metrics['separation'] < 0.2:
        suggestions.append("üí° Considere adicionar mais features ou ajustar contamination")
    
    if quality_metrics['stability'] < 0.5:
        suggestions.append("üí° Modelo pode precisar de mais dados de treino")
    
    if quality_metrics['confidence'] < 0.7:
        suggestions.append("üí° Considere usar ensemble ou ajustar hiperpar√¢metros")
    
    if suggestions:
        st.info("**Sugest√µes para melhorar o modelo:**")
        for suggestion in suggestions:
            st.write(suggestion)
```

### **6. Design System Consistente:**

**Diferencial**: Interface profissional com design system completo

```python
# Componentes reutiliz√°veis
class UIComponents:
    @staticmethod
    def metric_card(title, value, delta=None, color="blue"):
        colors = {
            "blue": "#3b82f6",
            "green": "#10b981", 
            "red": "#ef4444",
            "amber": "#f59e0b"
        }
        
        st.markdown(f"""
        <div style="
            background: linear-gradient(135deg, {colors[color]}15, {colors[color]}05);
            border: 1px solid {colors[color]}30;
            border-radius: 12px;
            padding: 1.5rem;
            text-align: center;
        ">
            <h3 style="color: {colors[color]}; margin: 0; font-size: 1.8rem;">{value}</h3>
            <p style="color: #64748b; margin: 0.5rem 0 0 0;">{title}</p>
        </div>
        """, unsafe_allow_html=True)
    
    @staticmethod
    def section_header(title, description=None):
        st.markdown(f"""
        <div style="
            background: linear-gradient(90deg, #1e40af, #7c3aed);
            background-clip: text;
            -webkit-background-clip: text;
            -webkit-text-fill-color: transparent;
            font-size: 2.2rem;
            font-weight: 700;
            margin: 2rem 0 1rem 0;
            border-bottom: 2px solid #e2e8f0;
            padding-bottom: 0.5rem;
        ">{title}</div>
        """, unsafe_allow_html=True)
        
        if description:
            st.markdown(f"*{description}*")
```

---

## 11. üöÄ **COMO EXECUTAR**

### **Pr√©-requisitos:**

```bash
# Python 3.8+
python --version

# Depend√™ncias principais
pip install streamlit pandas numpy plotly scikit-learn openpyxl seaborn matplotlib
```

### **Instala√ß√£o:**

```bash
# 1. Clone ou baixe os arquivos
git clone <repositorio> ou baixe os arquivos

# 2. Navegue para o diret√≥rio
cd "Tradicional Bet"

# 3. Instale depend√™ncias
pip install -r requirements.txt  # Se existir
# OU instale manualmente:
pip install streamlit pandas numpy plotly scikit-learn openpyxl seaborn matplotlib

# 4. Execute a aplica√ß√£o
streamlit run streamlit_app.py
```

### **Estrutura de Dados Esperada:**

O sistema espera um arquivo Excel (.xlsx) com as seguintes colunas:

```python
required_columns = {
    'data': 'Data/hora da transa√ß√£o (formato Excel)',
    'jogador_id': 'ID √∫nico do jogador',
    'jogo': 'Nome do jogo',
    'fornecedor': 'Fornecedor/provedor do jogo',
    'tipo': 'Tipo de jogo (slots, mesa, etc)',
    'aposta': 'Valor apostado (num√©rico)',
    'ganho': 'Valor ganho (num√©rico)',
    'ggr': 'Gross Gaming Revenue (opcional - ser√° calculado)'
}

# Exemplo de dados v√°lidos:
data_example = {
    'data': '2024-01-15 14:30:00',
    'jogador_id': 12345,
    'jogo': 'Book of Ra',
    'fornecedor': 'Novomatic',
    'tipo': 'Slot',
    'aposta': 10.00,
    'ganho': 15.50
}
```

### **Configura√ß√µes Dispon√≠veis:**

```python
# No sidebar da aplica√ß√£o:
configuracoes = {
    'deteccao_anomalias': True,  # Ativar ML para anomalias
    'sistema_recomendacao': True,  # Ativar sistema de recomenda√ß√£o
    'analise_detalhada': True,  # An√°lises avan√ßadas
    'taxa_anomalias': 0.05,  # 5% de anomalias esperadas
    'componentes_svd': 30,  # Dimens√µes latentes para SVD
    'periodo_analise': 'Todos'  # Filtro temporal
}
```

### **Navega√ß√£o no Dashboard:**

1. **üìä Upload**: Fa√ßa upload do arquivo Excel na sidebar
2. **‚öôÔ∏è Configura√ß√µes**: Ajuste par√¢metros do ML na sidebar  
3. **üìà An√°lise Temporal**: Evolu√ß√£o dos KPIs ao longo do tempo
4. **üèÜ Performance**: Rankings e an√°lises comparativas
5. **üó∫Ô∏è Heatmaps**: Correla√ß√µes e padr√µes em mapas de calor
6. **üîç ML - Anomalias**: Detec√ß√£o autom√°tica de comportamentos suspeitos
7. **üéØ ML - Recomenda√ß√µes**: Sistema de recomenda√ß√£o personalizado
8. **üë§ An√°lise Individual**: Deep dive em jogadores espec√≠ficos

### **Exporta√ß√£o de Dados:**

```python
# Dados dispon√≠veis para exporta√ß√£o:
exportacoes_disponiveis = {
    'transacoes_suspeitas.csv': 'Transa√ß√µes identificadas como an√¥malas',
    'recomendacoes_jogadores.csv': 'Recomenda√ß√µes personalizadas por jogador',
    'metricas_performance.csv': 'KPIs e m√©tricas de neg√≥cio',
    'segmentacao_jogadores.csv': 'Segmenta√ß√£o RFM dos jogadores'
}
```

### **Solu√ß√£o de Problemas Comuns:**

```python
# 1. Erro de formato de data
# Solu√ß√£o: Certifique-se que a coluna 'data' est√° em formato Excel v√°lido

# 2. Erro de mem√≥ria com arquivos grandes
# Solu√ß√£o: Processe em lotes menores ou aumente mem√≥ria dispon√≠vel

# 3. Modelos ML n√£o executando
# Solu√ß√£o: Verifique se h√° dados suficientes (m√≠n. 100 transa√ß√µes)

# 4. Gr√°ficos n√£o carregando
# Solu√ß√£o: Atualize o navegador ou use um navegador moderno

# 5. Formata√ß√£o brasileira n√£o funcionando
# Solu√ß√£o: Sistema tem fallback autom√°tico se locale n√£o dispon√≠vel
```

---

## üìö **REFER√äNCIAS E BIBLIOGRAFIA**

### **Algoritmos de Machine Learning:**
- Liu, F. T., Ting, K. M., & Zhou, Z. H. (2008). "Isolation forest." ICDM.
- Koren, Y., Bell, R., & Volinsky, C. (2009). "Matrix factorization techniques for recommender systems." Computer.
- Golub, G. H., & Reinsch, C. (1970). "Singular value decomposition and least squares solutions." Numerische mathematik.

### **M√©tricas de Avalia√ß√£o:**
- Herlocker, J. L., Konstan, J. A., Terveen, L. G., & Riedl, J. T. (2004). "Evaluating collaborative filtering recommender systems." ACM TOIS.
- Aggarwal, C. C. (2016). "Recommender systems: the textbook." Springer.
- Chandola, V., Banerjee, A., & Kumar, V. (2009). "Anomaly detection: A survey." ACM computing surveys.

### **Gaming Industry Analytics:**
- Griffiths, M. (2003). "Internet gambling: Issues, concerns, and recommendations." CyberPsychology & Behavior.
- LaPlante, D. A., & Shaffer, H. J. (2007). "Understanding the influence of gambling opportunities." Journal of gambling studies.

---

**üîÑ Vers√£o: 1.0 - Documenta√ß√£o Completa**  
**üìß Para d√∫vidas: Consulte a documenta√ß√£o oficial do Streamlit e Scikit-learn**

---

*Este documento representa uma explica√ß√£o t√©cnica completa do sistema TRADICIONAL BET Dashboard, desde os fundamentos matem√°ticos dos algoritmos at√© os detalhes de implementa√ß√£o e casos de uso pr√°ticos.*